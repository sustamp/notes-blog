---
layout: post-custom
title: 哈希算法笔记
---

## 1.什么是哈希
Hash也叫散列、哈希。原理是把**任意长度**的输入，通过哈希算法变成**固定长度**的输出。

这个映射的方法就是对应的Hash算法。而映射后得到的二进制串就是Hash值。

MD5、SHA1、SHA256、SHA512等都是Hash算法。

## 2.哈希的特点

1. Hash值**无法**反向**推导**出原始数据。
2. 输入数据的微小变化会得到完全不同的hash值。
3. 相同的输入会得到相同的输出。
4. hash算法的执行效率要**高效**，长文本也能快速地计算出Hash值。
5. hash算法的**冲突**概率**要小**。

> **抽屉原理**：桌上有10盒巧克力，要放到9个抽屉中。无论怎么放，至少有一个抽屉会放不少于2盒的巧克力。这个现象就是"抽屉原理"。  
> 抽屉原理又是也叫鸽巢原理，它是组合数学的一个重要原理。


## 3.哈希碰撞的解决方案
前面提到的抽屉原理，hash算法是有一定冲突的。

解决冲突的常用方法是：
- 链地址法
- 开放寻址法

### 3.1 链地址法
链地址法的处理处理逻辑如下：
- 使用一个**链表**数组，来**存储**数据。
- 当多个键值对映射到同一个哈希位置时（哈希冲突），这些键值对会被存储在同一个链表中。
- 每个链表的节点可以是一个键值对或指向下一个节点的指针。
  
优点：
- 实现简单，不会因为表的大小限制而导致性能下降。
  
缺点：
- 链表较长时，查询效率会降低。


### 3.2 开放寻址法
用大小为 M 的**数组**保存 N 个键值对，其中M > N。依靠数组中的**空位**解决哈希冲突。基于这种策略的所有方法被统称为**开放地址**哈希表。

常用的开放寻址法有：
- 线性探测
- 二次探测
- 双重哈希

#### 线性探测
从冲突位置开始，依次检查下一个位置，直到找到空位或查遍全表。

简单来说：**一旦发生冲突，就去寻找下一个空位，只要散列表足够大，空的散列地址总能找到**。

线性探测法的数学描述： 

`h(k, i) = (h(k, 0) + i) mod m`

- `i`: 表示当前进行的是第几轮探查。
  - i=1时，即是探查h(k, 0)的下一个；
  - i=2，即是再下一个。
- `mod m`: 到达了表的底下之后，回到顶端从头开始。


#### 二次探测
从冲突位置开始，以一定的步长（通常是平方数）进行探测，直到找到空位。

#### 双重哈希
使用两个哈希函数，第一个哈希函数确定初始位置，第二个哈希函数确定步长，直到找到空位。

开放寻址法的优点：
- 优点是数据都存储在哈希表中，不需要额外空间。
- 缺点是表的利用率较高时，**插入**和**查找**的**效率**会显著**下降**。（因为某些数字不放在余数位置）


## 哈希算法的应用

### 信息加密
用户敏感信息加密，密码加密。

而今MD5，SHA1已经不安全了，该类公开算法已经被创建了明文密文对应的查询数据库：

[https://www.cmd5.com/](https://www.cmd5.com/)

记录条数90万亿，占用磁盘空间500TB，查询成功率95%以上。


### 数据校验
使用`- git commit`命令都会有一个`commit id`，比如：

```
19d02d2cc358e59b3d04f82677dbf3808ae4fc40
```

`commit id` 内容主要包括：
- tree哈希
- parent哈希
- 作者信息
- 本次提交备注

使用针对这些信息进行 `SHA-1` 算法后得到值就是本次提交的 `commit id`。

> Linux kernel开创者和Git的开发者——Linus说，Git使用了`sha1`并非是为了安全性，而是为了**数据的完整性**。它可以保证，在很多年后，你重新checkout某个commit时，一定是它多年前的当时的状态，完全一摸一样，完全值得信任。

但最新研究表明，理论上对其进行哈希碰撞（hash collision，不同的两块数据有相同的hash值）的攻击可以在2^51（2的51次方）左右的次数内实现。  
不过由于commit id 是针对单个仓库里的，所以实际应用中我们可以认为如果两个文件的SHA-1值是相同的，那么它们确是完全相同的内容。

#### 版权校验
- 版权的保护或者违禁信息的打击：
  - 第一个用户上传的时候，我们认为是版权所有者，计算一个hash值存下来。
  - 当第二个用户上传的时候，同样计算hash值，如果hash值一样的话，就算同一个文件。

遇到玩家重复上传同一张图片或者视频的情况，使用这种校验的方式，可以有效减少cos服务的存储空间。

这种方案其实也给用户传播违禁文件提高了一些门槛，不是简单的换一个名字或者改一下后缀名就可以躲避掉打击了。  

当然这种方式也是可以绕过的，**图片**的你随便改一下**颜色**，**视频**去掉一**帧**就又是完全不同的hash值。

#### 大文件分块校验
进行文件传输时，为了提高传输效率和可靠性，通常会把一个大文件拆分成很多小的数据块各自传输，这样做的好处是：如果某个小的数据块在传输过程中损坏了，只要重新下载这个块就好。

每个数据库在传输前会计算其哈希值，接收方在接收到所有数据块后，通过比对哈希值来验证数据的完整性和正确性。具体步骤大概如下：
1. 文件分块：
   - 将大文件按照固定大小（如1MB）分割成多个小的数据块。
   - 每个数据块可以编号，以便于管理和重组。
2. 计算哈希值：
   - 对每个数据块分别计算哈希值（如MD5、SHA-256等）。
   - 将每个数据块的哈希值记录在一个哈希列表（Hash List）中。
3. 传输数据块：
   - 将每个数据块及其对应的哈希值传输到接收方。
   - 如果某个数据块在传输过程中损坏或丢失，接收方可以根据哈希值请求重新传输该数据块。
4. 接收和校验：
   - 接收方接收到所有数据块后，重新计算每个数据块的哈希值。
   - 将重新计算的哈希值与接收到的哈希列表中的哈希值进行比对：
     - 如果所有哈希值匹配，则说明数据传输成功且完整；
     - 如果有不匹配的哈希值，则说明数据块在传输过程中出现了问题，需要重新传输。
  

以上步骤如果在文件分块特别多的时候，遍历对比的效率会比较低。为了进一步提高校验效率，可以在进行优化：

5. 优化：Root Hash：
   - 可以把所有分块的hash值**组合**成一个大的**字符串**，对于这个字符串再做一次**哈希运算运算**，得到一个**根哈希值**（Root hash）。
   - 接收方在接收到所有数据块后，首**先验证Root Hash**是否匹配。
   - 如果根哈希值匹配，**再逐一验证**每个**数据块**的哈希值。

   
举例：

假设有一个100MB的大文件，我们需要将其传输到另一台服务器上。
1. 文件分块：将文件分成100个1MB的数据块。
2. 计算哈希值：
   - 对每个1MB的数据块分别计算SHA-256哈希值。
   - 记录每个数据块的哈希值到哈希列表中。

```py
import hashlib

def calculate_hash(data):
    return hashlib.sha256(data).hexdigest()

file_path = 'large_file.bin'
block_size = 1024 * 1024  # 1MB
hash_list = []

with open(file_path, 'rb') as f:
    block_number = 1
    while True:
        data = f.read(block_size)
        if not data:
            break
        block_hash = calculate_hash(data)
        hash_list.append((block_number, block_hash))
        block_number += 1
```

3. 传输数据块：
   - 将每个数据块及其对应的哈希值传输到接收方。
   - 将所有数据块的哈希值组合成大字符串，计算`Root Hash`。

```py
send_blocks = []  # 假设这是要传输的数据块
# 计算Root Hash
root_hash = calculate_hash(''.join([hash_value for _, hash_value in hash_list]))
```

4. 接收和校验：
   - 接收方接收到所有数据块后，首先验证`Root Hash`是否匹配。
   - 如果根哈希值匹配，重新计算每个数据块的哈希值，并与接收到的哈希列表进行比对。

```py
received_blocks = []  # 假设这是接收到的数据块
received_hashes = {}  # 假设这是接收到的哈希值

# 验证Root Hash
received_root_hash = calculate_hash(''.join([hash_value for _, hash_value in received_hashes.items()]))

if received_root_hash == root_hash:
    print("Root hash matches, all blocks are verified successfully.")
else:
    print("Root hash does not match, some blocks may be corrupted or missing.")

# 逐一验证每个数据块的哈希值
for block_number, received_data in received_blocks:
    calculated_hash = calculate_hash(received_data)
    expected_hash = received_hashes.get(block_number)
    if calculated_hash != expected_hash:
        print(f"Block {block_number} is corrupted or missing.")
    else:
        print(f"Block {block_number} is verified successfully.")
```

**思考**：为什么根哈希值匹配了，还要再验证每个数据块的哈希值？

为了确保数据的绝对完整性和可靠性，通常还是建议验证每个数据块的哈希值。以下是一些原因：

1. 额外的安全性
   
即使根哈希值匹配，也不能完全排除某些数据块在传输过程中发生了细微的变化。虽然这种情况极为罕见，但在某些高安全性的应用场景中，额外的验证可以提供更高的保障。

2. 错误定位

如果根哈希值不匹配，接收方需要知道具体是哪个数据块出了问题。通过逐一验证每个数据块的哈希值，可以快速定位到具体的错误数据块，从而进行针对性的重新传输。

3. 性能考虑

在大多数情况下，根哈希值的验证是一个快速的过程。如果根哈希值匹配，再逐一验证每个数据块的哈希值也不会增加太多额外的开销。相反，如果根哈希值不匹配，直接进行逐个验证可以节省时间和资源。

4. 数据完整性

在某些应用场景中，数据的完整性至关重要。例如，在金融交易、医疗记录等领域，任何细微的数据差异都可能导致严重的后果。因此，即使根哈希值匹配，仍然需要进行详细的验证以确保数据的绝对正确。


**优化建议**

尽管如此，可以通过一些优化措施来提高效率：
1. 并行验证：
   - 在验证每个数据块的哈希值时，可以使用多线程或多进程并行处理，以提高验证速度。
2. 分批验证：
   - 可以将数据块分成多个批次，先验证每个批次的根哈希值，再逐一验证每个批次内的数据块。


### 负载均衡

#### 哈希取模
对高星级业务量大的情况下，都会采用分库分表实现负载均衡，最简单的做法就是利用分库数量或分表数量进行哈希取余。得到的余数就是该数据要分配的节点。

但业务扩展迅猛需要再增加或减少节点时，这种做法会导致**几乎所有的数据**要**重新分配**。大量的数据搬迁会引发不少问题，特别是引起大量缓存失效，从而引发缓存雪崩。

#### 一致性哈希（哈希环）
为解决节点变更容易出现的大量数据重新分配的问题，利用一致性哈希算法进行处理。

一致性哈希算法的基本原理是：
- 将输入的值进行hash运算，对结果hash值进行2^32取模，这里和普通的hash取模算法不一样的点是在一致性hash算法里将取模的结果映射到一个环上，并且要分配的节点也同样进行相应的hash运算映射到环上。
- 将缓存节点与被缓存对象都映射到hash环上以后，从被缓存对象的位置出发，沿顺时针方向遇到的第一个服务节点，就是当前对象将要缓存的节点。

<img src="/assets/pictures/diagram-consistent-hash.png" alt="一致性哈希示意图" />

由于被缓存对象与服务器hash后的值是固定的，所以，在服务器不变的情况下，一个缓存对象的key必定会被缓存到固定的服务器上，那么，当下次想要访问这个用户的数据时，只要再次使用相同的算法进行计算，即可算出这个用户的数据被缓存在哪个服务器上，直接去对应的服务器查找对应的数据即可。

当服务器缩容或扩容时，影响的只是某一**部分**的数据范围要**重新分配**，大大减少了数据迁移的量。

但一致性哈希算法也有缺点，当节点数量较少时，会有**数据倾斜**，数据大多被分配在某一个节点上。

#### 哈希槽

## 哈希算法的拓展应用

### SimHash

### GeoHash

### 布隆过滤器